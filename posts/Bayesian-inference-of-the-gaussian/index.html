<!DOCTYPE html><html lang="en" > <!-- The Head v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Bayesian inference for the Gaussian | ZHU Chen</title><meta name="generator" content="Jekyll v4.1.1" /><meta property="og:title" content="Bayesian inference for the Gaussian" /><meta name="author" content="ZHU Chen" /><meta property="og:locale" content="en_US" /><meta name="description" content="Known variance \( \sigma^2 \) and unknown mean \( \mu \)1 Consider a set of \( N \) observations \( \mathbf{X} = {x_1, \cdots, x_n} \) from a Gaussian, where the variance \(\sigma^2 \) is known, and we want to infer the mean \(\mu \). The likelihood function is therefore can be written as \begin{align} p(\mathbf{X}| \mu) = \prod_{n=1}^{N} p(x_n|\mu) = \frac{1}{(2\pi\sigma^2)^{N/2}}\exp{\left(-\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_n-\mu)^2\right)} \end{align} Note the likelihood function is not a probability distribution function over \(\mu \), because it is not normalized. We choose the prior \(p(\mu)\) given by a Gaussian as well so that it can be a conjugate distribution for the likelihood function. In this case, the posterior will also be a Gaussian. The prior distribution is chosen as PRML 2.3.6 &#8617;" /><meta property="og:description" content="Known variance \( \sigma^2 \) and unknown mean \( \mu \)1 Consider a set of \( N \) observations \( \mathbf{X} = {x_1, \cdots, x_n} \) from a Gaussian, where the variance \(\sigma^2 \) is known, and we want to infer the mean \(\mu \). The likelihood function is therefore can be written as \begin{align} p(\mathbf{X}| \mu) = \prod_{n=1}^{N} p(x_n|\mu) = \frac{1}{(2\pi\sigma^2)^{N/2}}\exp{\left(-\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_n-\mu)^2\right)} \end{align} Note the likelihood function is not a probability distribution function over \(\mu \), because it is not normalized. We choose the prior \(p(\mu)\) given by a Gaussian as well so that it can be a conjugate distribution for the likelihood function. In this case, the posterior will also be a Gaussian. The prior distribution is chosen as PRML 2.3.6 &#8617;" /><link rel="canonical" href="https://zhujohn9604.github.io/johnzhu.blog/posts/Bayesian-inference-of-the-gaussian/" /><meta property="og:url" content="https://zhujohn9604.github.io/johnzhu.blog/posts/Bayesian-inference-of-the-gaussian/" /><meta property="og:site_name" content="ZHU Chen" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-08-07T12:33:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Bayesian inference for the Gaussian" /><meta name="twitter:site" content="@akagami9604" /><meta name="twitter:creator" content="@ZHU Chen" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"Known variance \\( \\sigma^2 \\) and unknown mean \\( \\mu \\)1 Consider a set of \\( N \\) observations \\( \\mathbf{X} = {x_1, \\cdots, x_n} \\) from a Gaussian, where the variance \\(\\sigma^2 \\) is known, and we want to infer the mean \\(\\mu \\). The likelihood function is therefore can be written as \\begin{align} p(\\mathbf{X}| \\mu) = \\prod_{n=1}^{N} p(x_n|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{N/2}}\\exp{\\left(-\\frac{1}{2\\sigma^2}\\sum_{n=1}^{N}(x_n-\\mu)^2\\right)} \\end{align} Note the likelihood function is not a probability distribution function over \\(\\mu \\), because it is not normalized. We choose the prior \\(p(\\mu)\\) given by a Gaussian as well so that it can be a conjugate distribution for the likelihood function. In this case, the posterior will also be a Gaussian. The prior distribution is chosen as PRML 2.3.6 &#8617;","@type":"BlogPosting","headline":"Bayesian inference for the Gaussian","dateModified":"2020-08-07T12:33:00+09:00","url":"https://zhujohn9604.github.io/johnzhu.blog/posts/Bayesian-inference-of-the-gaussian/","datePublished":"2020-08-07T12:33:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://zhujohn9604.github.io/johnzhu.blog/posts/Bayesian-inference-of-the-gaussian/"},"author":{"@type":"Person","name":"ZHU Chen"},"@context":"https://schema.org"}</script> <!-- The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps Generated by: https://www.favicon-generator.org/ v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT license --><link rel="shortcut icon" href="/johnzhu.blog/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/johnzhu.blog/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/johnzhu.blog/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/johnzhu.blog/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/johnzhu.blog/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/johnzhu.blog/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/johnzhu.blog/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/johnzhu.blog/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/johnzhu.blog/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/johnzhu.blog/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/johnzhu.blog/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/johnzhu.blog/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/johnzhu.blog/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/johnzhu.blog/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/johnzhu.blog/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/johnzhu.blog/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/johnzhu.blog/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/johnzhu.blog/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/johnzhu.blog/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/johnzhu.blog/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="preload" as="style" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"> <!-- CSS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --><link rel="preload" as="style" href="/johnzhu.blog/assets/css/post.css"><link rel="stylesheet" href="/johnzhu.blog/assets/css/post.css"><link rel="preload" as="style" href="/johnzhu.blog/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/johnzhu.blog/assets/css/lib/bootstrap-toc.min.css" /><link rel="preload" as="script" href="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" async></script> <!-- JS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --> <script src="/johnzhu.blog/assets/js/post.min.js" async></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script> <script src="/johnzhu.blog/app.js" defer></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"> <!-- The Side Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/johnzhu.blog/" alt="avatar"> <img src="/johnzhu.blog/assets/img/sample/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/johnzhu.blog/">ZHU Chen</a></div><div class="site-subtitle font-italic">Statistics, Programming, Mathematics, and a big fan of science.</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/johnzhu.blog/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/johnzhu.blog/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/johnzhu.blog/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/johnzhu.blog/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/johnzhu.blog/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></li></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <span id="mode-toggle-wrapper"> <!-- Switch the mode between dark and light. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <i class="mode-toggle fas fa-sun" dark-mode-invisible></i> <i class="mode-toggle fas fa-moon" light-mode-invisible></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.mode != null) { if (this.mode == ModeToggle.DARK_MODE) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.mode != null) { if (self.mode == ModeToggle.DARK_MODE) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightkMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightkMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span> <span class="icon-border"></span> <a href="https://github.com/zhujohn9604" target="_blank"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/akagami9604" target="_blank"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:window.open('mailto:' + ['zhuchen.uic','outlook.com'].join('@'))" > <i class="fas fa-envelope"></i> </a> <a href="/johnzhu.blog/feed.xml" > <i class="fas fa-rss"></i> </a></div></div><!-- The Top Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/johnzhu.blog/"> Posts </a> </span> <span>Bayesian inference for the Gaussian</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"> <!-- Fixed kramdown code highlight rendering: https://github.com/penibelst/jekyll-compress-html/issues/101 https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901 --><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Bayesian inference for the Gaussian</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Fri, Aug 7, 2020, 12:33 PM +0900" > Aug 7 <i class="unloaded">2020-08-07T12:33:00+09:00</i> </span> by <span class="author"> ZHU Chen </span></div><div> Updated <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Tue, Aug 11, 2020, 12:59 PM +0800" > Aug 11 <i class="unloaded">2020-08-11T13:59:54+09:00</i> </span></div></div><div class="post-content"><h2 id="known-variance--sigma2--and-unknown-mean--mu-">Known variance \( \sigma^2 \) and unknown mean \( \mu \)<sup id="fnref:footnote" role="doc-noteref"><a href="#fn:footnote" class="footnote">1</a></sup></h2><p>Consider a set of \( N \) observations \( \mathbf{X} = {x_1, \cdots, x_n} \) from a Gaussian, where the variance \(\sigma^2 \) is known, and we want to infer the mean \(\mu \). The likelihood function is therefore can be written as \begin{align} p(\mathbf{X}| \mu) = \prod_{n=1}^{N} p(x_n|\mu) = \frac{1}{(2\pi\sigma^2)^{N/2}}\exp{\left(-\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_n-\mu)^2\right)} \end{align} Note the likelihood function is not a probability distribution function over \(\mu \), because it is not normalized. We choose the prior \(p(\mu)\) given by a Gaussian as well so that it can be a conjugate distribution for the likelihood function. In this case, the posterior will also be a Gaussian. The prior distribution is chosen as</p><p>\begin{align} p(\mu) = \mathcal{N}(\mu|\mu_0, \sigma^2_0) \end{align}</p><p>and the posterior distribution is given by \begin{align} p(\mu|\mathbf{X}) \propto p(\mathbf{X}| \mu)p(\mu) \end{align} \begin{align} p(\mu|\mathbf{X}) = \mathcal{N}(\mu|\mu_N, \sigma^2_N) \end{align} We can calculate the parameters of the posterior distribution using the technique called <code class="language-plaintext highlighter-rouge">completing the square</code>. Let’s focus on the exponetial terms in the posterior distribution \(p(\mu|\mathbf{X}) \), we have</p><p>\begin{align} -\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_n-\mu)^2 - \frac{1}{2\sigma^2_0}(\mu-\mu_0)^2 = - \frac{1}{2\sigma^2_N}(\mu-\mu_N)^2 \end{align}</p><p>The coefficients of \(\mu^2 \) and \(\mu \) on the <code class="language-plaintext highlighter-rouge">left-hand side</code> are given as \begin{align} -(\frac{N}{2\sigma^2} + \frac{1}{2\sigma_0^2})\mu^2 \end{align} and \begin{align} -(\frac{N\mu_{ML}}{\sigma^2} + \frac{\mu_0}{\sigma_0^2})\mu \end{align} respectively, where \begin{align} \mu_{ML} = \frac{1}{N}\sum_{n=1}^{N}x_n \end{align} Similarly, the coefficients of \(\mu^2 \) and \(\mu \) on the <code class="language-plaintext highlighter-rouge">right-hand side</code> are given as \( -\frac{1}{2\sigma_{N}^2}\mu^2 \) and \(\frac{\mu_N}{\sigma_{N}^2}\mu \), respectively. From above, we can be easily get \(\mu_N\) and \(\sigma^2\) by comparision, and we have \begin{align} \mu_N = \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2}\mu_{ML} + \frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\mu_0 \end{align} \begin{align} \frac{1}{\sigma^2_{N}} = \frac{N}{\sigma^2} + \frac{1}{\sigma^2_0} \end{align}</p><h3 id="discussion">Discussion</h3><p>One may notice that when the number of observed data points \(N=0 \), the mean \(\mu_N \) of the posterior distribution will reduce to the prior mean \(\mu\). If we have infinite observations (\(N \rightarrow \infty \)), then the posterior mean will approach to the maximum likelihood estimator. For the posterior variance \(\sigma^2_N\), as we have infinite observations, the precision (inverse of the variance) will go to infinity, and the posterior distribution, therefore, becomes peaked around the maximum likelihood estimator.</p><h2 id="unknown-variance--sigma2--and-known-mean--mu-">Unknown variance \( \sigma^2 \) and known mean \( \mu \)</h2><p>Let’s then suppose the mean \( \mu \) is known and try to infer variance \( \sigma^2 \). Likewise, we first write down the likelihood function in terms of precision \(\lambda = 1/\sigma^2 \) as \begin{align} p(\mathbf{X}|\lambda) = \prod_{n=1}^{N} N(x_n|\mu, \lambda^{-1}) = \lambda^{N/2}\exp{\left( -\frac{\lambda}{2}\sum_{n=1}^{N}(x_n - \mu)^2 \right)} \end{align} In order to select a conjugate prior for the likelihood function, we first observe the components in it. The likelihood function consists of a power of \(\lambda \) and the exponential of linear function of \(\lambda \), so the gamma distribution might be a proper choice. The prior distribution is therefore given as \begin{align} p(\lambda|a_0, b_0) \propto \lambda^{a_{0}-1}\exp{(-b_0\lambda)} \end{align} and the posterior distribution is given by \begin{align} p(\lambda|\mathbf{X}) = {\rm Gam}(\lambda|a_N, b_N) \propto \lambda^{a_{0} + \frac{N}{2} -1}\exp{\left[ -\left(\frac{1}{2}\sum_{n=1}^{N}(x_n-\mu)^2 + b_0\right) \lambda \right]} \end{align} Unlike the Gaussian distribution, we can easily get the parameters of the gamma by observing its function form, which \begin{align} a_N = a_{0} + \frac{N}{2} \end{align}</p><p>\begin{align} b_N = \frac{1}{2}\sum_{n=1}^{N}(x_n-\mu)^2 + b_0 = \frac{N}{2}\sigma^2_{ML} + b_0 \end{align}</p><p>where \(\sigma^2_{ML} = \frac{1}{N} \sum_{n=1}^{N} (x_n-\mu)^2\).</p><h3 id="discussion-1">Discussion</h3><p>From the expression of \(a_N\), we notice that \(N \) subsequent observations contribute \(N/2 \) to the coefficient \(a \). Thus, one can interpret the \(a_0\) in the prior as \(2a_0\) <code class="language-plaintext highlighter-rouge">effective prior observations</code>. Similarly, from the expression of \(b_N\) we can see that \(N \) subsequent observations increase the coefficient \(b \) by \(\frac{N}{2}\sigma^2_{ML}\). Thus, the variance of the <code class="language-plaintext highlighter-rouge">effective prior observation</code> can be calculated by the total variance \(2b_0 \) divided by the number of effective prior observations \(2a_0\), which is \(2b_0 / 2a_0 = b_0/a_0\).</p><h2 id="unknown-variance--sigma2--and-unknown-mean--mu-">Unknown variance \( \sigma^2 \) and unknown mean \( \mu \)</h2><p>Finally, let’s consider both the mean and the variance are unknown. Similarly, we first write down the likelihood function in terms of mean \(\mu\) and precision \(\lambda\) as \begin{align} p(\mathbf{X}|\mu, \lambda) = \left(\frac{\lambda}{2\pi}\right)^{N/2}\exp{\left(-\frac{\lambda}{2}\sum_{n=1}^{N}(x_n - \mu)^2\right)} \end{align} \begin{align} \propto \left[\lambda^{1/2}\exp{\left(-\frac{\lambda \mu^2}{2} \right)} \right]^{N}\exp{\left(-\frac{\lambda}{2}\sum_{n=1}^{N}x_n^2 + \lambda\mu\sum_{n=1}^{N}x_n \right)} \end{align} By observing the form of By observing the form of likelihood function, the conjugate prior should take the form as \begin{align} p(\mu, \lambda) \propto \left[\lambda^{1/2}\exp{\left(-\frac{\lambda \mu^2}{2} \right)} \right]^{\beta}\exp{\left(-d \lambda + c\lambda\mu \right)} \end{align} \begin{align} =\exp{\left[-\frac{\beta \lambda}{2}(\mu-c/\beta)^2 \right]}\lambda^{\beta/2}\exp{\left[-\left(d-\frac{c^2}{2\beta} \right)\lambda \right]} \end{align} By inspecting into the form of the \(p(\mu, \lambda)\), we notice that it may consist of two components; one from a Gaussian and the other from a Gamma distribution. Formally, the prior can take the form as \begin{align} p(\mu, \lambda) = \mathcal{N}(\mu| c/\beta, (\beta\lambda)^{-1}){\rm Gam}(\lambda| \beta/2+1, d-c^2/(2\beta)) \end{align} which is called as a <code class="language-plaintext highlighter-rouge">Gaussian-gamma distribution</code>. One need to notice that these two are not independent to each other, becauase the precision of \(\mu \) is a linear function of \(\lambda \).</p><h2 id="extension-to-the-multivariate-case">Extension to the multivariate case</h2><p>In the case of multivariate Gaussian distribution \(\mathcal{N}(\mathbf{x}|\mathbf{\mu}, \mathbf{\Lambda^{-1}})\) for a \(D\)-dimensional \(\mathbf{x}\). When the mean \(\mathbf{\mu}\) is known and we want to infer the precision matrix \(\mathbf{\Lambda}\), the likelihood function is given as \begin{align} p(\mathbf{X}|\mathbf{\Lambda}) = \prod_{n=1}^{N} \mathcal{N}(\mathbf{x}|\mathbf{\mu}, \mathbf{\Lambda^{-1}}) \end{align} \begin{align} \propto |\Lambda|^{N/2}\exp{\left[-\frac{1}{2}\sum_{n=1}^{N}(\mathbf{x}_n-\mathbf{\mu})^{T}\mathbf{\Lambda}(\mathbf{x}_n-\mathbf{\mu}) \right]} \end{align}</p><p>\begin{align} = |\Lambda|^{N/2}\exp{\left[-\frac{1}{2}\sum_{n=1}^{N} {\rm Tr} ((\mathbf{x}_n-\mathbf{\mu})^{T}\mathbf{\Lambda}(\mathbf{x}_n-\mathbf{\mu})) \right]} \end{align}</p><p>\begin{align} = |\Lambda|^{N/2}\exp{\left[-\frac{1}{2}\sum_{n=1}^{N} {\rm Tr} ((\mathbf{x}_n-\mathbf{\mu})(\mathbf{x}_n-\mathbf{\mu})^{T}\mathbf{\Lambda}) \right]} \end{align}</p><p>\begin{align} = |\Lambda|^{N/2}\exp{\left[-\frac{1}{2} {\rm Tr} (\sum_{n=1}^{N}(\mathbf{x}_n-\mathbf{\mu})(\mathbf{x}_n-\mathbf{\mu})^{T}\mathbf{\Lambda}) \right]} \end{align}</p><p>\begin{align} = |\Lambda|^{N/2}\exp{\left[-\frac{1}{2} {\rm Tr} (\mathbf{S} \mathbf{\Lambda}) \right]} \end{align} where \(\mathbf{S}= \sum_{n=1}^{N}(\mathbf{x}_n-\mathbf{\mu})(\mathbf{x}_n-\mathbf{\mu})^{T}\).</p><p>A conjugate prior therefore comes from the <code class="language-plaintext highlighter-rouge">Wishart</code> distribution, which is given as \begin{align} \mathcal{W}(\mathbf{\Lambda}|\mathbf{W}, \mathcal{v}) \propto |\Lambda|^{(\mathcal{v}-D-1)/2}\exp{\left[-\frac{1}{2} {\rm Tr} (\mathbf{W}^{-1} \mathbf{\Lambda}) \right]} \end{align}</p><p>The posterior distribution is then given as \begin{align} p(\mathbf{\Lambda}|\mathbf{X}, \mathbf{W}, \mathcal{v}) \propto p(\mathbf{X}|\mathbf{\Lambda}) \mathcal{W}(\mathbf{\Lambda}|\mathbf{W}, \mathcal{v}) \end{align}</p><p>\begin{align} \propto |\Lambda|^{(\mathcal{v} + N -D-1)/2}\exp{\left[-\frac{1}{2} {\rm Tr} ((\mathbf{S}+\mathbf{W}^{-1}) \mathbf{\Lambda}) \right]} \end{align} where \(\mathcal{v}_N = \mathcal{v} + N\) and \(\mathbf{W}_N = (\mathbf{S}+\mathbf{W}^{-1})^{-1}\).</p><h4 id="reference-chapter">Reference chapter</h4><div class="footnotes" role="doc-endnotes"><ol><li id="fn:footnote" role="doc-endnote"><p>PRML 2.3.6 <a href="#fnref:footnote" class="reversefootnote" role="doc-backlink">&#8617;</a></p></li></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/johnzhu.blog/categories/prml/'>PRML</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/johnzhu.blog/tags/prml/" class="post-tag no-text-decoration" >PRML</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><!-- Post sharing snippet v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Bayesian inference for the Gaussian - ZHU Chen&url=https://zhujohn9604.github.io/johnzhu.blog/posts/Bayesian-inference-of-the-gaussian/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Bayesian inference for the Gaussian - ZHU Chen&u=https://zhujohn9604.github.io/johnzhu.blog/posts/Bayesian-inference-of-the-gaussian/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Bayesian inference for the Gaussian - ZHU Chen&url=https://zhujohn9604.github.io/johnzhu.blog/posts/Bayesian-inference-of-the-gaussian/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><!-- The Pannel on right side (Desktop views) v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><h3 data-toc-skip>Recent Update</h3><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/johnzhu.blog/posts/Bayesian-inference-of-the-gaussian/">Bayesian inference for the Gaussian</a></li><li><a href="/johnzhu.blog/posts/BERT/">An implementation of BERT</a></li><li><a href="/johnzhu.blog/posts/Disapear-of-antisymmetric/">Antisymmetic component disapears in the quadratic form</a></li></ul></div><div id="access-tags"><h3 data-toc-skip>Trending Tags</h3><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/johnzhu.blog/tags/prml/">PRML</a> <a class="post-tag" href="/johnzhu.blog/tags/nlp/">NLP</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><h3 data-toc-skip class="pl-3 pt-2 mb-2">Contents</h3><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <!-- Recommend the other 3 posts according to the tags and categories of the current post, if the number is not enough, use the other latest posts to supplement. v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/johnzhu.blog/posts/Disapear-of-antisymmetric/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Jul 12 <i class="unloaded">2020-07-12T12:33:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Antisymmetic component disapears in the quadratic form</h3><div class="text-muted small"><p> In the multivariate Gaussian distribution, the term in the exponent has the form \begin{align} (\mathbf{x} - \mathbf{\mu})^{T}\Sigma(\mathbf{x} - \mathbf{\mu}) \end{align} Note that the precision m...</p></div></div></a></div><div class="card"> <a href="/johnzhu.blog/posts/BERT/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Aug 11 <i class="unloaded">2020-08-11T11:33:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>An implementation of BERT</h3><div class="text-muted small"><p> This is an implementation of the Google BERT model [paper] in Pytorch. To better understand the BERT model details, I decided to write my own codes [github], and I was strongly inspired by HuggingF...</p></div></div></a></div></div></div><!-- Navigation buttons at the bottom of the post. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --><div class="post-navigation d-flex justify-content-between"> <a href="/johnzhu.blog/posts/Disapear-of-antisymmetric/" class="btn btn-outline-primary"><p>Antisymmetic component disapears in the quadratic form</p></a> <a href="/johnzhu.blog/posts/BERT/" class="btn btn-outline-primary"><p>An implementation of BERT</p></a></div><!-- The Disqus lazy loading. Powered by: https://osvaldas.info/lazy-loading-disqus-comments v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung MIT License --><div id="disqus" class="pt-2 pb-2"><p class="font-italic text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/johnzhu.blog/assets/js/lib/jquery.disqusloader.min.js"></script> <script> var options = { scriptUrl: '//johnzhu-blog.disqus.com/embed.js', disqusConfig: function() { this.page.url = 'https://zhujohn9604.github.io/johnzhu.blog/posts/Bayesian-inference-of-the-gaussian/'; this.page.identifier = 'johnzhu-blog'; } }; $.disqusLoader('#disqus', options); </script></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script> <!-- The Footer v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2020 <a href="https://twitter.com/akagami9604">ZHU Chen</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank">Jekyll</a>.</p></div></div></footer></div><!-- The Search results v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/johnzhu.blog/tags/prml/">PRML</a> <a class="post-tag" href="/johnzhu.blog/tags/nlp/">NLP</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <!-- The GA snippet v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', '', 'auto'); ga('send', 'pageview'); </script> <!-- Jekyll Simple Search loader v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/johnzhu.blog/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://zhujohn9604.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
