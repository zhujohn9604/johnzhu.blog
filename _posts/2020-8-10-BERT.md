---
title: "An implementation of BERT"
author: ZHU Chen
date: 2020-08-1 10:33:00 +0800
categories: [NLP]
tags: [NLP]
math: true
---
This is an implementation of the Google BERT model [[paper]](https://arxiv.org/abs/1810.04805) in Pytorch. To better understand the BERT model details, I decided to write my own codes [[github]](https://github.com/zhujohn9604/BERT), and I was strongly inspired by [HuggingFace's Implementation](https://github.com/huggingface/transformers). (Note: their implementations can be adapted to a lot of Transformer-based models, so it might be hard to read through; my implementation is only for BERT.)

## Example usage

### BertTokenizer
BertTokenizer is based on [WordPiece](https://arxiv.org/abs/1609.08144), and check the list of the pretrained tokenizers [here](https://github.com/zhujohn9604/BERT/blob/master/bert/tokenization_bert.py). To initialize a pretrained tokenizer and tokenize a given sentence:
```python
from tokenization_bert import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
text = "In the early days of the COVID-19 pandemic, public health officials believed that one of the most effective ways to fight the spread of the virus was to disinfect highly touched surfaces."
text_tokenized = tokenizer._tokenize(text)
print(text_tokenized)
```
If we want to feed the tokenized results into the model, use the `encode` method:
```python
from tokenization_bert import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
text = "In the early days of the COVID-19 pandemic, public health officials believed that one of the most effective ways to fight the spread of the virus was to disinfect highly touched surfaces."
text_encoded = tokenizer.encode(text, max_length=64, padding=True,
                                       truncation=True, encode_plus=True, return_tensors=True)
print(text_encoded)
```
You can find an complete example based on patent data [here](https://github.com/zhujohn9604/BERT/blob/master/bert/datasets.py).

### Fine-tuning BertModels
If we want to fine-tune BertModels, we need to load pretrained weights (Note: the pretrained weights should match the pretrained tokenizer). An example of configuring a text classification model is given by
```python
from configuration_bert import BertConfig
from modeling_bert import BertForSequenceClassification

config = BertConfig()
config.num_labels = 4
config.gradient_checkpointing = True
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased', config=config)
```
If you only have a GPU with the memory of 12GB, remember to turn on the `config.gradient_checkpointing` option. For details about `checkpointing`, please refer to [[blog]](https://qywu.github.io/2019/05/22/explore-gradient-checkpointing.html), [[paper]](https://arxiv.org/abs/1604.06174), and a good [illustration](https://github.com/cybertronai/gradient-checkpointing).

Finally, let's see how to use the fine-tuned model to do prediction:
```python
# a toy example
import torch
from tokenization_bert import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
text = "In the early days of the COVID-19 pandemic, public health officials believed that one of the most effective ways to fight the spread of the virus was to disinfect highly touched surfaces."
text_encoded = tokenizer.encode(text, max_length=64, padding=True,
                                       truncation=True, encode_plus=True, return_tensors=True)
toy_label = 1

loss, logits = model(text_encoded['input_ids'], attention_mask=text_encoded['attention_mask'], labels=torch.tensor([[toy_label]]))
print(loss, logits)
```
An complete example of patent classification can be found [here](https://github.com/zhujohn9604/BERT/blob/master/bert/train.py). 